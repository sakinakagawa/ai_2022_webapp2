{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk39HKZ4cqgp",
        "outputId": "c19de33c-829a-454e-a032-050a7705df78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: icrawler in /usr/local/lib/python3.8/dist-packages (0.6.6)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.8/dist-packages (from icrawler) (2.25.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from icrawler) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from icrawler) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from icrawler) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from icrawler) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.9.1->icrawler) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.9.1->icrawler) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.9.1->icrawler) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.9.1->icrawler) (2022.12.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://storage.mantan-web.jp/images/2018/01/09/20180109dog00m200024000c/001_size9.jpg\n",
            "ERROR:downloader:Response status code 403, file https://storage.mainichikirei.jp/images/2018/07/03/20180703dog00m100010000c/001_size9.jpg\n",
            "ERROR:downloader:Response status code 403, file https://cdn.v2ph.com/photos/cGQT1TCsmaGMcn9e.jpg\n",
            "ERROR:downloader:Response status code 403, file https://storage.mantan-web.jp/images/2021/04/15/20210415dog00m200027000c/001_size6.jpg\n",
            "ERROR:downloader:Response status code 404, file https://cdn.hk01.com/di/media/images/3643251/org/5895144701487842a9b6fcf9dbcd24ab.jpg\n",
            "ERROR:downloader:Response status code 403, file https://storage.mantan-web.jp/images/2018/11/07/20181107dog00m200034000c/001_size9.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.crank-in.net/img/db/1165203_1200.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.crank-in.net/img/db/1193438_650.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.crank-in.net/img/db/1181433_650.jpg\n",
            "ERROR:downloader:Response status code 403, file https://storage.mantan-web.jp/images/2019/06/19/20190619dog00m200072000c/008_size9.jpg\n"
          ]
        }
      ],
      "source": [
        "!pip install icrawler\n",
        "\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "\n",
        "# ①石原さとみ×100\n",
        "crawler = BingImageCrawler(storage={\"root_dir\": \"石原さとみ\"})\n",
        "crawler.crawl(keyword=\"石原さとみ\", max_num=100)\n",
        "\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "\n",
        "# ②新垣結衣×100\n",
        "crawler = BingImageCrawler(storage={\"root_dir\": \"新垣結衣\"})\n",
        "crawler.crawl(keyword=\"新垣結衣\", max_num=100)\n",
        "\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "\n",
        "# ⑤北川景子×100\n",
        "crawler = BingImageCrawler(storage={\"root_dir\": \"北川景子\"})\n",
        "crawler.crawl(keyword=\"北川景子\", max_num=100)\n",
        "\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "\n",
        "# ⑥小松菜奈×100\n",
        "crawler = BingImageCrawler(storage={\"root_dir\": \"小松菜奈\"})\n",
        "crawler.crawl(keyword=\"小松菜奈\", max_num=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "classes = [\"石原さとみ\", \"新垣結衣\", \"北川景子\", \"小松菜奈\"]\n",
        "num_classes = len(classes)\n",
        "image_size = 64\n",
        "num_testdata = 25\n",
        "\n",
        "X_train = []\n",
        "X_test  = []\n",
        "y_train = []\n",
        "y_test  = []\n",
        "\n",
        "for index, classlabel in enumerate(classes):\n",
        "    photos_dir = \"./\" + classlabel\n",
        "    files = glob.glob(photos_dir + \"/*.jpg\")\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image)\n",
        "        if i < num_testdata:\n",
        "            X_test.append(data)\n",
        "            y_test.append(index)\n",
        "        else:\n",
        "\n",
        "            for angle in range(-20, 20, 5):\n",
        "\n",
        "                img_r = image.rotate(angle)\n",
        "                data = np.asarray(img_r)\n",
        "                X_train.append(data)\n",
        "                y_train.append(index)\n",
        "                img_trains = img_r.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                data = np.asarray(img_trains)\n",
        "                X_train.append(data)\n",
        "                y_train.append(index)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test  = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test  = np.array(y_test)\n",
        "\n",
        "xy = (X_train, X_test, y_train, y_test)\n",
        "np.save(\"./女優.npy\", xy)"
      ],
      "metadata": {
        "id": "V57nER4Wcxma"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}